{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TFWALSMovieLens.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNuZ2/XntsSkW0qa1Fqwf0g"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"7eX2qoUdF2br","colab_type":"code","outputId":"b20564eb-6b35-4f55-c074-d889e735fd46","executionInfo":{"status":"ok","timestamp":1590576338710,"user_tz":-180,"elapsed":25737,"user":{"displayName":"Rıdvan Sirma","photoUrl":"","userId":"13305846716776645290"}},"colab":{"base_uri":"https://localhost:8080/","height":120}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E6IPTbFsHTDb","colab_type":"code","outputId":"fa40cd15-afcb-4254-fca5-d2231e2102bb","executionInfo":{"status":"ok","timestamp":1590576963113,"user_tz":-180,"elapsed":1310,"user":{"displayName":"Rıdvan Sirma","photoUrl":"","userId":"13305846716776645290"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["import pandas as pd\n","import numpy as np\n","\n","user_data_path = '/content/drive/My Drive/ml-100k/u.data'\n","r_cols = ['user_id', 'movie_id', 'rating']\n","ratings = pd.read_csv(user_data_path, usecols=[0,1,2], sep='\\t', names=r_cols, encoding='latin-1')\n","nmovie = np.max(ratings[\"movie_id\"]) + 1\n","nuser = np.max(ratings[\"user_id\"]) + 1\n","ratings.describe()\n","\n","'''filter = ratings[\"user_id\"] == 55\n","  \n","# filtering data \n","ratings.where(filter, inplace = True) \n","ratings = ratings.dropna()\n","ratings = ratings.sort_values('movie_id')\n","  \n","# display \n","ratings'''"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'filter = ratings[\"user_id\"] == 55\\n  \\n# filtering data \\nratings.where(filter, inplace = True) \\nratings = ratings.dropna()\\nratings = ratings.sort_values(\\'movie_id\\')\\n  \\n# display \\nratings'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"5QqGLf6fJDtx","colab_type":"code","outputId":"41db9071-69ef-4514-dafd-724d5264b0b6","executionInfo":{"status":"ok","timestamp":1590577115931,"user_tz":-180,"elapsed":5649,"user":{"displayName":"Rıdvan Sirma","photoUrl":"","userId":"13305846716776645290"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["%tensorflow_version 1.x\n","import tensorflow as tf"],"execution_count":4,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1To7A58QzZqR","colab_type":"code","outputId":"4a002b01-00aa-48d9-e071-1920150c4fea","executionInfo":{"status":"ok","timestamp":1590577115932,"user_tz":-180,"elapsed":3461,"user":{"displayName":"Rıdvan Sirma","photoUrl":"","userId":"13305846716776645290"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["tf.__version__"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.15.2'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"t11Ewry2I8Gg","colab_type":"code","colab":{}},"source":["grouped_by_items = ratings.groupby(\"movie_id\")\n","with tf.python_io.TFRecordWriter(\"/content/drive/My Drive/users_for_item\") as ofp:\n","    for item, grouped in grouped_by_items:\n","        example = tf.train.Example(features = tf.train.Features(feature = {\n","            \"key\": tf.train.Feature(int64_list = tf.train.Int64List(value = [item])),\n","            \"indices\": tf.train.Feature(int64_list = tf.train.Int64List(value = grouped[\"user_id\"].values)),\n","            \"values\": tf.train.Feature(float_list = tf.train.FloatList(value = grouped[\"rating\"].values))\n","        }))\n","        ofp.write(example.SerializeToString())\n","grouped_by_users = ratings.groupby(\"user_id\")\n","with tf.python_io.TFRecordWriter(\"/content/drive/My Drive/items_for_user\") as ofp:\n","    for user, grouped in grouped_by_users:\n","        example = tf.train.Example(features = tf.train.Features(feature = {\n","            \"key\": tf.train.Feature(int64_list = tf.train.Int64List(value = [user])),\n","            \"indices\": tf.train.Feature(int64_list = tf.train.Int64List(value = grouped[\"movie_id\"].values)),\n","            \"values\": tf.train.Feature(float_list = tf.train.FloatList(value = grouped[\"rating\"].values))\n","        }))\n","        ofp.write(example.SerializeToString())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMIs6jceDRFO","colab_type":"code","colab":{}},"source":["from tensorflow.python.lib.io import file_io\n","from tensorflow.contrib.factorization import WALSMatrixFactorization\n","import os\n","  \n","def read_dataset(mode, args):\n","    def decode_example(protos, vocab_size):\n","        features = {\n","            \"key\": tf.FixedLenFeature(shape = [1], dtype = tf.int64),\n","            \"indices\": tf.VarLenFeature(dtype = tf.int64),\n","            \"values\": tf.VarLenFeature(dtype = tf.float32)}\n","        parsed_features = tf.parse_single_example(serialized = protos, features = features)\n","        values = tf.sparse_merge(sp_ids = parsed_features[\"indices\"], sp_values = parsed_features[\"values\"], vocab_size = vocab_size)\n","        # Save key to remap after batching\n","        # This is a temporary workaround to assign correct row numbers in each batch.\n","        # You can ignore details of this part and remap_keys().\n","        key = parsed_features[\"key\"]\n","        decoded_sparse_tensor = tf.SparseTensor(indices = tf.concat(values = [values.indices, [key]], axis = 0), \n","                                                values = tf.concat(values = [values.values, [0.0]], axis = 0), \n","                                                dense_shape = values.dense_shape)\n","        return decoded_sparse_tensor\n","  \n","  \n","    def remap_keys(sparse_tensor):\n","        # Current indices of our SparseTensor that we need to fix\n","        bad_indices = sparse_tensor.indices # shape = (current_batch_size * (number_of_items/users[i] + 1), 2)\n","        # Current values of our SparseTensor that we need to fix\n","        bad_values = sparse_tensor.values # shape = (current_batch_size * (number_of_items/users[i] + 1),)\n","\n","        # Since batch is ordered, the last value for a batch index is the user\n","        # Find where the batch index chages to extract the user rows\n","        # 1 where user, else 0\n","        user_mask = tf.concat(values = [bad_indices[1:,0] - bad_indices[:-1,0], tf.constant(value = [1], dtype = tf.int64)], axis = 0) # shape = (current_batch_size * (number_of_items/users[i] + 1), 2)\n","\n","        # Mask out the user rows from the values\n","        good_values = tf.boolean_mask(tensor = bad_values, mask = tf.equal(x = user_mask, y = 0)) # shape = (current_batch_size * number_of_items/users[i],)\n","        item_indices = tf.boolean_mask(tensor = bad_indices, mask = tf.equal(x = user_mask, y = 0)) # shape = (current_batch_size * number_of_items/users[i],)\n","        user_indices = tf.boolean_mask(tensor = bad_indices, mask = tf.equal(x = user_mask, y = 1))[:, 1] # shape = (current_batch_size,)\n","\n","        good_user_indices = tf.gather(params = user_indices, indices = item_indices[:,0]) # shape = (current_batch_size * number_of_items/users[i],)\n","\n","        # User and item indices are rank 1, need to make rank 1 to concat\n","        good_user_indices_expanded = tf.expand_dims(input = good_user_indices, axis = -1) # shape = (current_batch_size * number_of_items/users[i], 1)\n","        good_item_indices_expanded = tf.expand_dims(input = item_indices[:, 1], axis = -1) # shape = (current_batch_size * number_of_items/users[i], 1)\n","        good_indices = tf.concat(values = [good_user_indices_expanded, good_item_indices_expanded], axis = 1) # shape = (current_batch_size * number_of_items/users[i], 2)\n","\n","        remapped_sparse_tensor = tf.SparseTensor(indices = good_indices, values = good_values, dense_shape = sparse_tensor.dense_shape)\n","        return remapped_sparse_tensor\n","\n","    \n","    def parse_tfrecords(filename, vocab_size):\n","        if mode == tf.estimator.ModeKeys.TRAIN:\n","            num_epochs = None # indefinitely\n","        else:\n","            num_epochs = 1 # end-of-input after this\n","\n","        files = tf.gfile.Glob(filename = os.path.join(args[\"input_path\"], filename))\n","\n","        # Create dataset from file list\n","        dataset = tf.data.TFRecordDataset(files)\n","        dataset = dataset.map(map_func = lambda x: decode_example(x, vocab_size))\n","        dataset = dataset.repeat(count = num_epochs)\n","        dataset = dataset.batch(batch_size = args[\"batch_size\"])\n","        dataset = dataset.map(map_func = lambda x: remap_keys(x))\n","        return dataset.make_one_shot_iterator().get_next()\n","  \n","    def _input_fn():\n","        features = {\n","            WALSMatrixFactorization.INPUT_ROWS: parse_tfrecords(\"items_for_user\", args[\"nitems\"]),\n","            WALSMatrixFactorization.INPUT_COLS: parse_tfrecords(\"users_for_item\", args[\"nusers\"]),\n","            WALSMatrixFactorization.PROJECT_ROW: tf.constant(True)\n","        }\n","        return features, None\n","\n","    return _input_fn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ayavcd2M3Zup","colab_type":"code","colab":{}},"source":["def find_top_k(user, item_factors, k):\n","    all_items = tf.matmul(a = tf.expand_dims(input = user, axis = 0), b = tf.transpose(a = item_factors))\n","    topk = tf.nn.top_k(input = all_items, k = k)\n","    return tf.cast(x = topk.indices, dtype = tf.int64)\n","    \n","def batch_predict(args):\n","    with tf.Session() as sess:\n","        estimator = tf.contrib.factorization.WALSMatrixFactorization(\n","            num_rows = args[\"nusers\"], \n","            num_cols = args[\"nitems\"],\n","            embedding_dimension = args[\"n_embeds\"],\n","            model_dir = args[\"output_dir\"])\n","        \n","        # This is how you would get the row factors for out-of-vocab user data\n","        # row_factors = list(estimator.get_projections(input_fn=read_dataset(tf.estimator.ModeKeys.EVAL, args)))\n","        # user_factors = tf.convert_to_tensor(np.array(row_factors))\n","\n","        # But for in-vocab data, the row factors are already in the checkpoint\n","        user_factors = tf.convert_to_tensor(value = estimator.get_row_factors()[0]) # (nusers, nembeds)\n","        # In either case, we have to assume catalog doesn\"t change, so col_factors are read in\n","        item_factors = tf.convert_to_tensor(value = estimator.get_col_factors()[0])# (nitems, nembeds)\n","        # For each user, find the top K items\n","        topk = tf.squeeze(input = tf.map_fn(fn = lambda user: find_top_k(user, item_factors, args[\"topk\"]), elems = user_factors, dtype = tf.int64))\n","        with file_io.FileIO(os.path.join(args[\"output_dir\"], \"batch_pred.txt\"), mode = 'w') as f:\n","            for best_items_for_user in topk.eval():\n","                f.write(\",\".join(str(x) for x in best_items_for_user) + '\\n')\n","\n","def train_and_evaluate(args):\n","    train_steps = int(0.5 + (1.0 * args[\"num_epochs\"] * args[\"nusers\"]) / args[\"batch_size\"])\n","    steps_in_epoch = int(0.5 + args[\"nusers\"] / args[\"batch_size\"])\n","    print(\"Will train for {} steps, evaluating once every {} steps\".format(train_steps, steps_in_epoch))\n","    def experiment_fn(output_dir):\n","        return tf.contrib.learn.Experiment(\n","            tf.contrib.factorization.WALSMatrixFactorization(\n","                num_rows = args[\"nusers\"], \n","                num_cols = args[\"nitems\"],\n","                embedding_dimension = args[\"n_embeds\"],\n","                model_dir = args[\"output_dir\"]),\n","            train_input_fn = read_dataset(tf.estimator.ModeKeys.TRAIN, args),\n","            eval_input_fn = read_dataset(tf.estimator.ModeKeys.EVAL, args),\n","            train_steps = train_steps,\n","            eval_steps = 1,\n","            min_eval_frequency = steps_in_epoch\n","        )\n","\n","    from tensorflow.contrib.learn.python.learn import learn_runner\n","    learn_runner.run(experiment_fn = experiment_fn, output_dir = args[\"output_dir\"])\n","    \n","    #batch_predict(args)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmWM_-c1XsJA","colab_type":"code","outputId":"a9638abd-6041-4346-f979-0ebcc73ee1cf","executionInfo":{"status":"ok","timestamp":1590577208281,"user_tz":-180,"elapsed":5401,"user":{"displayName":"Rıdvan Sirma","photoUrl":"","userId":"13305846716776645290"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train_and_evaluate({\n","    \"output_dir\": \"wals_trained\",\n","    \"input_path\": \"/content/drive/My Drive/\",\n","    \"num_epochs\": 0.05,\n","    \"nitems\": nmovie,\n","    \"nusers\": nuser,\n","\n","    \"batch_size\": 256,\n","    \"n_embeds\": 20,\n","    \"topk\": 3\n","  })"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Will train for 0 steps, evaluating once every 4 steps\n","WARNING:tensorflow:From <ipython-input-13-cc78ae0737a7>:47: run (from tensorflow.contrib.learn.python.learn.learn_runner) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.estimator.train_and_evaluate.\n","INFO:tensorflow:Using default config.\n","INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3caab2d550>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n","  per_process_gpu_memory_fraction: 1.0\n","}\n",", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'wals_trained', '_session_creation_timeout_secs': 7200}\n","WARNING:tensorflow:From <ipython-input-13-cc78ae0737a7>:43: Experiment.__init__ (from tensorflow.contrib.learn.python.learn.experiment) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please switch to tf.estimator.train_and_evaluate. You will also have to convert to a tf.estimator.Estimator.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/monitors.py:279: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n","Instructions for updating:\n","Monitors are deprecated. Please use tf.train.SessionRunHook.\n","WARNING:tensorflow:Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3caab19a60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING: Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3caab19a60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING:tensorflow:From <ipython-input-12-d587af809bfb>:12: sparse_merge (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","No similar op available at this time.\n","WARNING:tensorflow:Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca7c28158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING: Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca7c28158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From <ipython-input-12-d587af809bfb>:64: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n","WARNING:tensorflow:Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca7c28d08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING: Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca7c28d08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING:tensorflow:Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca7c597b8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING: Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca7c597b8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/factorization/python/ops/wals.py:315: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into wals_trained/model.ckpt.\n","INFO:tensorflow:SweepHook running init op.\n","INFO:tensorflow:SweepHook running prep ops for the row sweep.\n","INFO:tensorflow:Next fit step starting.\n","INFO:tensorflow:loss = 1319505.1, step = 1\n","INFO:tensorflow:Saving checkpoints for 1 into wals_trained/model.ckpt.\n","INFO:tensorflow:Loss for final step: 1319505.1.\n","WARNING:tensorflow:Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca77ac378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING: Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca77ac378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING:tensorflow:Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca77ac1e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING: Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca77ac1e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING:tensorflow:Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca77d8158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING: Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca77d8158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING:tensorflow:Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca77d8e18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING: Entity <function read_dataset.<locals>.parse_tfrecords.<locals>.<lambda> at 0x7f3ca77d8e18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","INFO:tensorflow:Starting evaluation at 2020-05-27T11:00:06Z\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from wals_trained/model.ckpt-1\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Evaluation [1/1]\n","INFO:tensorflow:Finished evaluation at 2020-05-27-11:00:07\n","INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 1319505.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"POWqOceP6a3B","colab_type":"code","colab":{}},"source":["#!rm -rf wals_trained"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5i6wNDF73Yr4","colab_type":"code","outputId":"a09ad03a-492b-4f57-dafa-5ef3e964889d","executionInfo":{"status":"ok","timestamp":1590577658363,"user_tz":-180,"elapsed":864,"user":{"displayName":"Rıdvan Sirma","photoUrl":"","userId":"13305846716776645290"}},"colab":{"base_uri":"https://localhost:8080/","height":237}},"source":["with tf.Session() as sess:\n","    estimator = tf.contrib.factorization.WALSMatrixFactorization(\n","        num_rows = nuser, \n","        num_cols = nmovie,\n","        embedding_dimension = 20,\n","        model_dir = \"wals_trained\")\n","    \n","    user_factors = tf.convert_to_tensor(value = estimator.get_row_factors()[0])\n","    item_factors = tf.convert_to_tensor(value = estimator.get_col_factors()[0])\n","    print('##########################')\n","    print(user_factors[5].eval())\n","    print('##########################')\n","    print(find_top_k(user_factors[5], item_factors, 3).eval())"],"execution_count":18,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using default config.\n","INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3ca4253978>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {\n","  per_process_gpu_memory_fraction: 1.0\n","}\n",", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'wals_trained', '_session_creation_timeout_secs': 7200}\n","##########################\n","[ 0.15714577  0.18484986  0.1358616  -0.1172308   0.11673594 -0.0506761\n"," -0.06451725 -0.03856195 -0.05123179  0.21483706  0.05705332  0.1566962\n","  0.16422957  0.1290887   0.02715626  0.1167402   0.0565377  -0.07191438\n","  0.25389084  0.00151463]\n","##########################\n","[[446  98 193]]\n"],"name":"stdout"}]}]}